% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation Metrics}\label{chapter:evaluation_metrics}

\section{Introduction}
[kopi peyst]
In order to give a formal definition of the item recommendation task, we introduce the following notation. The set of users in the recommender system will be denoted by U, and the set of items by I. Moreover, we denote by R the set of ratings recorded in the system, and write S the set of possible values for a rating (e.g., S = [1..5] or S = {like; dislike}). Also, we suppose that no more than one rating can be made by any user u $\in$  U for a particular item i $\in$ I and write $r_ui$ this rating. To identify the subset of users that have rated an item i, we use the notation $U_ii$. Likewise, $I_u$ represents the subset of items that have been rated by a user u. Finally, the items that have been rated by two users u and v, i.e. $I_u$ $\cap$ $I_v$, is an important concept in our presentation, and we use Iuv to denote this concept. In a similar fashion, $U_ij$ is used to denote the set of users that have rated both items i and j.

Two of the most important problems associated with recommender systems are the rating prediction and top-N recommendation problems. The first problem is to predict the rating that a user u will give his or her unrated item i. When ratings are available, this task is most often defined as a regression or (multi-class) classification problem where the goal is to learn a function f $\colon$ U $\times$ I $\rightarrow$ S that predicts the rating f(u, i) of a user u for a new item i. Accuracy is commonly used to evaluate the performance of the recommendation method. Typically, the ratings R are divided into a training set $R_train$ used to learn f , and a test set Rtest used to evaluate the prediction accuracy. Two popular measures of accuracy are the Mean Absolute Error (MAE):

$$
\mathrm { MAE } ( f ) = \frac { 1 } { \left| \mathcal { R } _ {test} \right| } \sum _ { r _ { u i } \in \mathcal { R } _ {test} } \left| f ( u , i ) - r _ { u i } \right|
$$

and the Root Mean Squared Error (RMSE):

$$
\mathrm { RMSE } ( f ) = \sqrt { \frac { 1 } { \left| \mathcal { R } _ { test} \right| } \sum _ { r _ { i u } } \left( f ( u , i ) - r _ { u i } \right) ^ { 2 } }
$$

When ratings are not available, for instance, if only the list of items purchased by each user is known, measuring the rating prediction accuracy is not possible. In such cases, the problem of finding the best item is usually transformed into the task of recommending to an active user ua a list L($u_a$) containing N items likely to interest him or her [15, 59]. The quality of such method can be evaluated by splitting the items of I into a set Itrain, used to learn L, and a test set Itest. Let  T(u) $\in$ $I_u$ $\cap$ $I_test$ be the subset of test items that a user u found relevant. If the user responses are binary, these can be the items that u has rated positively. Otherwise, if only a list of purchased or accessed items is given for each user u, then these items can be used as T(u). The performance of the method is then computed using the measures of precision and recall:

$$
\mathrm { Precision } ( L ) = \frac { 1 } { | u | } \sum _ { u \in \mathcal { U } } | L ( u ) \cap T ( u ) | / | L ( u ) |
$$


$$
\mathrm { Recall } ( L ) = \frac { 1 } { | u | } \sum _ { u \in \mathcal { U } } | L ( u ) \cap T ( u ) | / | T ( u ) |
$$

A drawback of this task is that all items of a recommendation list L.u/ are considered equally interesting to user u. An alternative setting, described in [15], consists in learning a function L that maps each user u to a list L.u/ where items are ordered by their “interestingness” to u. If the test set is built by randomly selecting, for each user u, a single item iu of Iu, the performance of L can be evaluated with the Average Reciprocal Hit-Rank (ARHR):

$$
\mathrm { ARHR } ( L ) = \frac { 1 } { | \mathcal { U } | } \sum _ { u \in \mathcal { U } } \frac { 1 } { \mathrm { rank } \left( i _ { u } , L ( u ) \right) }
$$

where rank(iu ; L(u)) is the rank of item iu in L(u). A more extensive description of evaluation measures for recommender systems can be found in Chap. 8 of this book.

