@book{latex,
  title = "LaTeX : A Documentation Preparation System User's Guide and Reference Manual",
  publisher = "Addison-Wesley Professional",
  year = "1994",
  author = "Leslie Lamport"
}

@Inbook{Ricci2015,
	author="Ricci, Francesco
	and Rokach, Lior
	and Shapira, Bracha",
	editor="Ricci, Francesco
	and Rokach, Lior
	and Shapira, Bracha",
	title="Recommender Systems: Introduction and Challenges",
	bookTitle="Recommender Systems Handbook",
	year="2015",
	publisher="Springer US",
	address="Boston, MA",
	pages="1--34",
	abstract="Recommender Systems (RSs) are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user. In this introductory chapter, we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters included in this handbook. Additionally, we aim to help the reader navigate the rich and detailed content that this handbook offers.",
	isbn="978-1-4899-7637-6",
	doi="10.1007/978-1-4899-7637-6_1",
	url="https://doi.org/10.1007/978-1-4899-7637-6_1"
}

@article{PARK201210059,
	title = "A literature review and classification of recommender systems research",
	journal = "Expert Systems with Applications",
	volume = "39",
	number = "11",
	pages = "10059 - 10072",
	year = "2012",
	issn = "0957-4174",
	doi = "https://doi.org/10.1016/j.eswa.2012.02.038",
	url = "http://www.sciencedirect.com/science/article/pii/S0957417412002825",
	author = "Deuk Hee Park and Hyea Kyeong Kim and Il Young Choi and Jae Kyeong Kim",
	keywords = "Recommender systems, Literature review, Data mining technique, Classification"
}

@Article{Beel2016,
	author="Beel, Joeran
	and Gipp, Bela
	and Langer, Stefan
	and Breitinger, Corinna",
	title="Research-paper recommender systems: a literature survey",
	journal="International Journal on Digital Libraries",
	year="2016",
	month="11",
	day="01",
	volume="17",
	number="4",
	pages="305--338",
	abstract="In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.",
	issn="1432-1300",
	doi="10.1007/s00799-015-0156-0",
	url="https://doi.org/10.1007/s00799-015-0156-0"
}

@Article{sathya2013comparison,
	title="Comparison of supervised and unsupervised learning algorithms for pattern classification",
	author="Sathya, R and Abraham, Annamma",
	journal="International Journal of Advanced Research in Artificial Intelligence",
	volume="2",
	number="2",
	pages="34--38",
	year="2013",
	publisher="Citeseer"
}

@book{bird2009natural,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={" O'Reilly Media, Inc."}
}

